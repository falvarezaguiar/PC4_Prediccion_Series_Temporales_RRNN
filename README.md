# PC4 â€” PredicciÃ³n de Series Temporales con Redes Recurrentes (Rossmann)

## ğŸ“Œ DescripciÃ³n del Proyecto

Proyecto de consolidaciÃ³n del MÃ¡ster en Inteligencia Artificial (MBIT) enfocado en la **predicciÃ³n de ventas diarias** de tiendas Rossmann usando **redes recurrentes profundas (LSTM)**.

**Objetivo:** Construir un modelo que maximice el RÂ² en test, sin usar la variable `Customers` (informaciÃ³n futura).

---

## ğŸ“Š Resultados Finales

### Modelos LSTM (notebooks Colab)

| Tienda | Modelo | RÂ² Train | RÂ² Test | RMSE Test | Lookback |
|--------|--------|----------|---------|-----------|----------|
| **Tienda 1** | LSTM + OHE | 93.7% | **90.4%** âœ… | 718 | 7 dÃ­as |
| **Tienda 2** | LSTM + OHE | 86.7% | **80.0%** | 1027 | 28 dÃ­as |
| Tienda 1 | LSTM + Embeddings | 96.0% | 49.8% | â€” | 14 dÃ­as |
| Tienda 2 | LSTM + Embeddings | 85.1% | 71.4% | â€” | 7 dÃ­as |

> **Nota:** El enfoque con **One-Hot Encoding (OHE)** supera significativamente a los **Embeddings** en este dataset.

### Comparativa con Baselines (Tienda 1)

| Modelo | MAE (test) | RMSE (test) | RÂ² (test) |
|--------|------------|-------------|-----------|
| NaÃ¯ve (t-1) | 2172 | 3097 | -0.76 |
| Rolling Mean 7d | 1868 | 2423 | -0.07 |
| Linear Regression | 748 | 954 | 0.83 |
| Random Forest | 388 | 633 | 0.93 |
| **LSTM (OHE)** | â€” | **718** | **0.90** |

---

## ğŸ“ Estructura del Proyecto

```
PC4/
â”œâ”€â”€ ejemplos/                          # Notebooks de referencia (SARIMAX, AutoARIMA, LSTMs)
â”œâ”€â”€ PC_series_temporales/
â”‚   â”œâ”€â”€ data/                          # Datasets (CSVs)
â”‚   â”‚   â”œâ”€â”€ datos_diarios_tienda1.csv
â”‚   â”‚   â”œâ”€â”€ datos_diarios_tienda2.csv
â”‚   â”‚   â”œâ”€â”€ df_tienda1_preprocesado.csv
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚
â”‚   â”œâ”€â”€ notebook_colab/                # ğŸ”¥ NOTEBOOKS FINALES (Colab)
â”‚   â”‚   â”œâ”€â”€ PC4_Series_temporales_T1_LSTM_endÃ³gena_y_exÃ³genas.ipynb
â”‚   â”‚   â”œâ”€â”€ PC4_Series_temporales_T2_LSTM_endÃ³gena_y_exÃ³genas.ipynb
â”‚   â”‚   â”œâ”€â”€ PC4_series_temporales_T1_embeddings.ipynb
â”‚   â”‚   â””â”€â”€ PC4_series_temporales_T2_embeddings.ipynb
â”‚   â”‚
â”‚   â”œâ”€â”€ Store6_EDA_Visualizacion.ipynb          # ğŸ“ˆ EDA introductorio
â”‚   â”œâ”€â”€ Store6_Feature_Eng&Baselines.ipynb      # ğŸ”§ Feature Engineering + Baselines
â”‚   â”œâ”€â”€ PC4-Baseline2_LSTM.ipynb                # ğŸ§  Baseline LSTM (local)
â”‚   â””â”€â”€ metrics_table.csv                       # Tabla de mÃ©tricas baselines
â”‚
â””â”€â”€ README.md
```

---

## ğŸ““ DescripciÃ³n de Notebooks

### 1ï¸âƒ£ EDA & VisualizaciÃ³n (`Store6_EDA_Visualizacion.ipynb`)
**Entorno:** Local  
**PropÃ³sito:** Entender el dataset antes de modelar.

- Carga y validaciÃ³n de datos (881 registros, 2013-2015)
- Chequeos de calidad: duplicados, gaps temporales
- DescomposiciÃ³n STL, ACF/PACF, tests ADF/KPSS
- AnÃ¡lisis por dÃ­a de semana y mes
- DetecciÃ³n de outliers y patrones estacionales

---

### 2ï¸âƒ£ Feature Engineering & Baselines (`Store6_Feature_Eng&Baselines.ipynb`)
**Entorno:** Local  
**PropÃ³sito:** Crear features y establecer baselines de referencia.

**Baselines implementados:**
- **NaÃ¯ve:** Pred(t) = Real(t-1)
- **Rolling Mean 7d:** Media mÃ³vil de los Ãºltimos 7 dÃ­as
- **Linear Regression** y **Random Forest** como referencias ML

**Feature Engineering:**
- Variables de calendario: `year`, `month`, `day`, `dayofweek`, `is_weekend`
- CodificaciÃ³n cÃ­clica: `month_sin/cos`, `dow_sin/cos`
- One-Hot Encoding para categÃ³ricas (Promo, StateHoliday, etc.)
- VerificaciÃ³n de incongruencias (Customers/Open/Sales)

**ConclusiÃ³n:** Los baselines estadÃ­sticos tienen RÂ² negativo â†’ necesidad de modelos mÃ¡s avanzados.

---

### 3ï¸âƒ£ Baseline LSTM (`PC4-Baseline2_LSTM.ipynb`)
**Entorno:** Local  
**PropÃ³sito:** Primer modelo LSTM para comparaciÃ³n.

**ConfiguraciÃ³n:**
| ParÃ¡metro | Valor |
|-----------|-------|
| TIME_STEPS (lookback) | 30 |
| LSTM_UNITS | 64 |
| DROPOUT | 0.15 |
| BATCH_SIZE | 64 |
| EPOCHS_MAX | 500 (EarlyStopping) |
| Split | 80% train / 20% test |

**Variables de entrada (multivariante):**
- Target: `Sales` (escalado con StandardScaler)
- Binarias: `Open`, `Promo`, `SchoolHoliday`, `StateHoliday_*`
- CÃ­clicas: `day_year_sin/cos`, `dow_sin/cos`, `day_month_sin/cos`, `year_norm`

**MÃ©tricas:** MAE, sMAPE, RÂ²

---

### 4ï¸âƒ£ Notebooks Finales â€” Colab (`notebook_colab/`)
**Entorno:** Google Colab  
**PropÃ³sito:** Modelos optimizados con mejores resultados.

#### a) LSTM + One-Hot Encoding (âœ… Mejor resultado)
- `PC4_Series_temporales_T1_LSTM_endÃ³gena_y_exÃ³genas.ipynb`
- `PC4_Series_temporales_T2_LSTM_endÃ³gena_y_exÃ³genas.ipynb`

**Resultados Tienda 1:**
```
R2 del modelo en training      :  0.937
R2 del modelo en test          :  0.904
RMSE del modelo en test        :  718
```

**Resultados Tienda 2:**
```
R2 del modelo en training      :  0.867
R2 del modelo en test          :  0.800
RMSE del modelo en test        :  1027
```

**Enfoque:**
- One-Hot Encoding para `Month` (12 cols) y `DayOfWeek` (7 cols)
- Variables binarias: Open, Promo, SchoolHoliday, StateHoliday_*
- Enventanado: `lookback = 7` dÃ­as (T1) y `lookback = 28` dÃ­as (T2)
- Escalado: StandardScaler solo en target
- Arquitectura: LSTM(5 neuronas) â†’ Dense(1)
- Optimizador: Adam

#### b) LSTM + Embeddings (Alternativa)
- `PC4_series_temporales_T1_embeddings.ipynb`
- `PC4_series_temporales_T2_embeddings.ipynb`

**Resultados Tienda 1:**
```
R2 en training: 0.960
R2 en val:      0.878
R2 en test:     0.498
```

**Resultados Tienda 2:**
```
R2 en training: 0.851
R2 en val:      0.753
R2 en test:     0.714
```

**Enfoque:**
- Capas Embedding para variables categÃ³ricas (month, day_week, day_month)
- `dim_embedding = 3`
- Permite representaciones densas aprendidas
- **Resultado inferior al OHE** (posible overfitting en embeddings)

---

## ğŸ› ï¸ TecnologÃ­as Utilizadas

- **Python 3.x**
- **Keras / TensorFlow** â€” Redes LSTM
- **Scikit-learn** â€” Preprocesado, mÃ©tricas, baselines ML
- **Statsmodels** â€” STL, ACF/PACF, ADF/KPSS
- **Pandas / NumPy** â€” ManipulaciÃ³n de datos
- **Matplotlib / Seaborn** â€” VisualizaciÃ³n

---

## ğŸ“ˆ Datos

**Dataset:** Ventas diarias de tiendas Rossmann (Kaggle)  
**PerÃ­odo:** 2013-01-01 â†’ 2015-05-31 (~881 registros por tienda)

| Variable | DescripciÃ³n |
|----------|-------------|
| `Sales` | Ventas diarias (TARGET) |
| `Open` | Tienda abierta (0/1) |
| `Promo` | PromociÃ³n activa (0/1) |
| `SchoolHoliday` | Vacaciones escolares (0/1) |
| `StateHoliday_*` | Festivos estatales (one-hot) |
| `DayOfWeek` | DÃ­a de la semana (1-7) |
| `Customers` | âŒ No usar (informaciÃ³n futura) |

---

## ğŸš€ CÃ³mo Ejecutar

### Local (EDA, Feature Eng, Baseline2)
```bash
cd PC_series_temporales/
jupyter notebook Store6_EDA_Visualizacion.ipynb
```

### Google Colab (Notebooks finales)
1. Subir notebooks de `notebook_colab/` a Colab
2. Montar Google Drive cuando se solicite
3. Ejecutar celdas secuencialmente

---

## ğŸ“ Conclusiones

1. Los **baselines estadÃ­sticos** (NaÃ¯ve, Rolling Mean) son insuficientes (RÂ² < 0)
2. **ML clÃ¡sico** (Random Forest) alcanza RÂ² = 0.93 en Tienda 1
3. **LSTM con One-Hot Encoding** logra RÂ² = **0.90** en Tienda 1 y **0.80** en Tienda 2
4. **One-Hot Encoding supera a Embeddings** en este dataset (menos overfitting)
5. El enfoque **many-to-one** con ventana deslizante es efectivo para pronÃ³stico de 1 dÃ­a
6. La Tienda 2 presenta mayor dificultad de predicciÃ³n (posiblemente por patrones mÃ¡s errÃ¡ticos)

---

## ğŸ‘¤ Autor

**Francisco Alvarez**  
AdaptaciÃ³n de notebooks originales de Manuel SÃ¡nchez-MontaÃ±Ã©s

